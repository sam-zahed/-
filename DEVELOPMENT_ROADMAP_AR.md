# ๐บ๏ธ ุฎุงุฑุทุฉ ุทุฑูู ุงูุชุทููุฑ - ูุธุงู ูุณุงุนุฏุฉ ุงูููููููู

## ๐ฏ ุงูุฑุคูุฉ

ุชุทููุฑ ูุธุงู ุฐูู ุดุงูู ููููู ุงูููููููู ูู ุงูุนูุด ุจุดูู ูุณุชูู ูุขูู ุจุงุณุชุฎุฏุงู ุฃุญุฏุซ ุชูููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู.

---

## ๐ ุงูุญุงูุฉ ุงูุญุงููุฉ ูููุดุฑูุน

### โ ูุง ูุนูู ุงูุขู:
- โ ูุดู ุงูุฃุดูุงุก ุจุงุณุชุฎุฏุงู YOLO-World (60+ ูุฆุฉ)
- โ ุชุฑุฌูุฉ ุชููุงุฆูุฉ ููุนุฑุจูุฉ
- โ ุชูุฏูุฑ ุงููุณุงูุฉ ุงูุฃุณุงุณู
- โ ุงูุชุนุฑู ุนูู ุงููุฌูู ูุญูุธูุง
- โ ุชุญููู ุงูุตูุช ููุต (Whisper)
- โ ุชุญููู ุงููุต ูุตูุช (TTS)
- โ ูุงุนุฏุฉ ุจูุงูุงุช ููุฃุญุฏุงุซ ูุงูุฎุฑุงุฆุท
- โ WebSocket ููุงุชุตุงู ุงููุจุงุดุฑ
- โ API ูุงูู ูุน FastAPI

### โ๏ธ ูุง ูุญุชุงุฌ ุชุญุณูู:
- โ๏ธ ุฏูุฉ ุชูุฏูุฑ ุงููุณุงูุฉ (ุญุงููุงู ุชูุฑูุจูุฉ)
- โ๏ธ ุณุฑุนุฉ ุงูุงุณุชุฌุงุจุฉ (150-500ms)
- โ๏ธ ุฏุนู ุงููุบุฉ ุงูุนุฑุจูุฉ ูู TTS
- โ๏ธ ูุงุฌูุฉ ูุณุชุฎุฏู ููููุจุงูู
- โ๏ธ ูุธุงู ุงูุชูุจููุงุช ุงูุตูุชูุฉ
- โ๏ธ ุงูููุงุญุฉ ุงูุฏุงุฎููุฉ ูุงูุฎุงุฑุฌูุฉ

---

## ๐ ุงููุฑุญูุฉ 1: ุงูุชุญุณููุงุช ุงูููุฑูุฉ (ุฃุณุจูุน - ุดูุฑ)

### 1.1 ูุธุงู ุงูุชูุจููุงุช ุงูุตูุชูุฉ ุงูุฐูู โญโญโญ

**ุงูุฃููููุฉ:** ุนุงููุฉ ุฌุฏุงู  
**ุงูููุช ุงููุชููุน:** 3-5 ุฃูุงู  
**ุงูุตุนูุจุฉ:** ูุชูุณุทุฉ

**ุงููุตู:**
ูุธุงู ููุจู ุงููุณุชุฎุฏู ููุฑุงู ุจุงูุฃุฎุทุงุฑ ุญุณุจ ุงูุฃููููุฉ.

**ุงูุชูููุฐ:**

```python
# ููู ุฌุฏูุฏ: app/alerts/priority_system.py

from enum import Enum
from typing import List, Dict
import asyncio

class AlertPriority(Enum):
    CRITICAL = 1    # ุฎุทุฑ ููุฑู (ุฏุฑุฌุ ุญูุฑุฉุ ุณูุงุฑุฉ ูุฑูุจุฉ)
    HIGH = 2        # ุชุญุฐูุฑ ููู (ุดุฎุต ูุฑูุจุ ุจุงุจ)
    MEDIUM = 3      # ูุนูููุฉ ูููุฏุฉ (ุฃุซุงุซุ ุฃุดูุงุก)
    LOW = 4         # ูุนูููุฉ ุนุงูุฉ (ุฃููุงูุ ุชูุงุตูู)

PRIORITY_MAP = {
    # ุฃุฎุทุงุฑ ููุฑูุฉ
    'stairs': AlertPriority.CRITICAL,
    'staircase': AlertPriority.CRITICAL,
    'hole': AlertPriority.CRITICAL,
    'pothole': AlertPriority.CRITICAL,
    'escalator': AlertPriority.CRITICAL,
    
    # ุชุญุฐูุฑุงุช ุนุงููุฉ
    'car': AlertPriority.HIGH,
    'truck': AlertPriority.HIGH,
    'bus': AlertPriority.HIGH,
    'motorcycle': AlertPriority.HIGH,
    'bicycle': AlertPriority.HIGH,
    'door': AlertPriority.HIGH,
    'open door': AlertPriority.HIGH,
    
    # ูุชูุณุทุฉ
    'person': AlertPriority.MEDIUM,
    'child': AlertPriority.HIGH,  # ุทูู = ุฃููููุฉ ุนุงููุฉ
    'wall': AlertPriority.MEDIUM,
    'obstacle': AlertPriority.MEDIUM,
    
    # ููุฎูุถุฉ
    'chair': AlertPriority.LOW,
    'table': AlertPriority.LOW,
}

DISTANCE_MULTIPLIER = {
    # ูููุง ูุงู ุงูุดูุก ุฃูุฑุจุ ุฒุงุฏุช ุงูุฃููููุฉ
    0.5: 3,   # ุฃูู ูู ูุตู ูุชุฑ = ุถุงุนู ุงูุฃููููุฉ 3 ูุฑุงุช
    1.0: 2,   # ุฃูู ูู ูุชุฑ = ุถุงุนู ูุฑุชูู
    2.0: 1.5, # ุฃูู ูู ูุชุฑูู = ุถุงุนู 1.5
    5.0: 1,   # ุฃูุซุฑ ูู 5 ุฃูุชุงุฑ = ุนุงุฏู
}

ARABIC_ALERTS = {
    AlertPriority.CRITICAL: "ุชุญุฐูุฑ ุฎุทุฑ!",
    AlertPriority.HIGH: "ุงูุชุจู!",
    AlertPriority.MEDIUM: "ููุงุญุธุฉ:",
    AlertPriority.LOW: ""
}

DIRECTION_AR = {
    'front': 'ุฃูุงูู',
    'left': 'ุนูู ูุณุงุฑู',
    'right': 'ุนูู ููููู',
    'back': 'ุฎููู'
}

def calculate_direction(bbox, image_width):
    """ุญุณุงุจ ุงุชุฌุงู ุงูุดูุก"""
    center_x = (bbox[0] + bbox[2]) / 2
    
    if center_x < image_width * 0.3:
        return 'left'
    elif center_x > image_width * 0.7:
        return 'right'
    else:
        return 'front'

def get_alert_priority(obj_class: str, distance: float) -> int:
    """ุญุณุงุจ ุฃููููุฉ ุงูุชูุจูู"""
    base_priority = PRIORITY_MAP.get(obj_class, AlertPriority.LOW)
    
    # ุชุนุฏูู ุญุณุจ ุงููุณุงูุฉ
    for dist_threshold, multiplier in sorted(DISTANCE_MULTIPLIER.items()):
        if distance <= dist_threshold:
            priority_value = base_priority.value / multiplier
            return max(1, int(priority_value))
    
    return base_priority.value

def generate_alert_message(detection: Dict, image_width: int) -> Dict:
    """ุชูููุฏ ุฑุณุงูุฉ ุชูุจูู"""
    obj_class = detection['class']
    class_ar = detection.get('class_ar', obj_class)
    distance = detection['distance_m']
    bbox = detection['bbox']
    
    priority_level = get_alert_priority(obj_class, distance)
    priority = AlertPriority(priority_level)
    
    direction = calculate_direction(bbox, image_width)
    direction_ar = DIRECTION_AR[direction]
    
    alert_prefix = ARABIC_ALERTS[priority]
    
    # ุตูุงุบุฉ ุงูุฑุณุงูุฉ
    if distance < 1:
        distance_text = f"ูุฑูุจ ุฌุฏุงู ููู"
    elif distance < 2:
        distance_text = f"ุนูู ุจุนุฏ ูุชุฑ {direction_ar}"
    else:
        distance_text = f"ุนูู ุจุนุฏ {distance:.0f} ูุชุฑ {direction_ar}"
    
    message = f"{alert_prefix} {class_ar} {distance_text}"
    
    return {
        'priority': priority.value,
        'priority_name': priority.name,
        'message': message,
        'object': class_ar,
        'distance': distance,
        'direction': direction,
        'should_speak': priority.value <= 2  # ููุท CRITICAL ู HIGH
    }

async def process_detections_with_alerts(detections: List[Dict], image_width: int = 640):
    """ูุนุงูุฌุฉ ุงููุดููุงุช ูุชูููุฏ ุงูุชูุจููุงุช"""
    alerts = []
    
    for detection in detections:
        alert = generate_alert_message(detection, image_width)
        alerts.append(alert)
    
    # ุชุฑุชูุจ ุญุณุจ ุงูุฃููููุฉ
    alerts.sort(key=lambda x: (x['priority'], x['distance']))
    
    # ุงูุชูุจููุงุช ุงูุตูุชูุฉ (ููุท ุงูุฃูู)
    critical_alerts = [a for a in alerts if a['should_speak']]
    
    return {
        'all_alerts': alerts,
        'critical_alerts': critical_alerts,
        'speak_message': ' . '.join([a['message'] for a in critical_alerts[:3]])  # ุฃูู 3 ููุท
    }
```

**ุฏูุฌ ูู ุงููุธุงู:**

```python
# ูู app/infer/router.py

from app.alerts.priority_system import process_detections_with_alerts

@router.post('/realtime')
async def realtime_infer(request: InferRequest):
    # ... ุงูููุฏ ุงูุญุงูู ...
    
    # ุฅุถุงูุฉ ูุธุงู ุงูุชูุจููุงุช
    alerts_data = await process_detections_with_alerts(objects, image_width=640)
    
    return {
        "objects": objects,
        "alerts": alerts_data['all_alerts'],
        "speak_message": alerts_data['speak_message'],  # ุงูุฑุณุงูุฉ ุงูุตูุชูุฉ
        "TTC": 999.0,
        "scene_confidence": 0.8,
        "inference_time_ms": inference_time,
        "model_version": "yolov8-v1"
    }
```

**ุงููุงุฆุฏุฉ:**
- ๐ฏ ุงููุณุชุฎุฏู ูุณูุน ููุท ุงูุฃูู
- โก ุงุณุชุฌุงุจุฉ ููุฑูุฉ ููุฃุฎุทุงุฑ
- ๐ง ุฐูู ูู ุชุฑุชูุจ ุงูุฃููููุงุช

---

### 1.2 ุชุญุณูู ุชูุฏูุฑ ุงููุณุงูุฉ ุจุงุณุชุฎุฏุงู Depth Estimation โญโญโญ

**ุงูุฃููููุฉ:** ุนุงููุฉ  
**ุงูููุช ุงููุชููุน:** 5-7 ุฃูุงู  
**ุงูุตุนูุจุฉ:** ูุชูุณุทุฉ-ุนุงููุฉ

**ุงููุตู:**
ุงุณุชุฎุฏุงู ูููุฐุฌ MiDaS ุฃู Depth-Anything ูุชูุฏูุฑ ุงููุณุงูุฉ ุงูุญููููุฉ.

**ุงูุชูููุฐ:**

```python
# ููู ุฌุฏูุฏ: app/vision/depth_estimator.py

import torch
import cv2
import numpy as np
from pathlib import Path

class DepthEstimator:
    def __init__(self):
        self.model = None
        self.load_model()
    
    def load_model(self):
        """ุชุญููู ูููุฐุฌ MiDaS"""
        try:
            # ุงุณุชุฎุฏุงู MiDaS Small (ุณุฑูุน)
            self.model = torch.hub.load("intel-isl/MiDaS", "MiDaS_small")
            self.model.eval()
            
            self.transform = torch.hub.load("intel-isl/MiDaS", "transforms").small_transform
            
            print("โ Depth Estimator loaded")
        except Exception as e:
            print(f"โ๏ธ Depth Estimator failed: {e}")
    
    def estimate_depth(self, image_bytes):
        """ุชูุฏูุฑ ุงูุนูู ูู ุตูุฑุฉ"""
        if self.model is None:
            return None
        
        try:
            # ุชุญููู bytes ูุตูุฑุฉ
            arr = np.frombuffer(image_bytes, np.uint8)
            img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # ุชุญููู ูููููุฐุฌ
            input_batch = self.transform(img_rgb)
            
            # ุงูุงุณุชูุชุงุฌ
            with torch.no_grad():
                prediction = self.model(input_batch)
                prediction = torch.nn.functional.interpolate(
                    prediction.unsqueeze(1),
                    size=img.shape[:2],
                    mode="bicubic",
                    align_corners=False,
                ).squeeze()
            
            depth_map = prediction.cpu().numpy()
            
            return depth_map
            
        except Exception as e:
            print(f"โ๏ธ Depth estimation error: {e}")
            return None
    
    def get_object_distance(self, depth_map, bbox):
        """ุญุณุงุจ ูุณุงูุฉ ุดูุก ูู ุฎุฑูุทุฉ ุงูุนูู"""
        if depth_map is None:
            return None
        
        try:
            x1, y1, x2, y2 = map(int, bbox)
            
            # ุงุณุชุฎุฑุงุฌ ููุทูุฉ ุงูุดูุก
            object_region = depth_map[y1:y2, x1:x2]
            
            # ุญุณุงุจ ุงููุชูุณุท (ุฃู ุงููุณูุท ููุฏูุฉ)
            median_depth = np.median(object_region)
            
            # ุชุญููู ููุณุงูุฉ ุชูุฑูุจูุฉ ุจุงูุฃูุชุงุฑ
            # MiDaS ูุนุทู ููู ูุณุจูุฉุ ูุญุชุงุฌ ูุนุงูุฑุฉ
            # ูุฐู ูุนุงุฏูุฉ ุชูุฑูุจูุฉ
            distance_m = 10.0 / (median_depth + 1.0)
            
            return max(0.3, min(distance_m, 20.0))  # ุจูู 30cm ู 20m
            
        except Exception as e:
            print(f"โ๏ธ Distance calculation error: {e}")
            return None

# ุฅูุดุงุก instance ุนุงู
depth_estimator = DepthEstimator()
```

**ุงูุฏูุฌ:**

```python
# ูู app/vision/model.py

from .depth_estimator import depth_estimator

class WorldDetector:
    def detect(self, image_bytes):
        # ... ุงูููุฏ ุงูุญุงูู ...
        
        # ุชูุฏูุฑ ุงูุนูู
        depth_map = depth_estimator.estimate_depth(image_bytes)
        
        for box in r.boxes:
            # ... ุงูููุฏ ุงูุญุงูู ...
            
            # ุงุณุชุฎุฏุงู ุงูุนูู ุงูุญูููู ุฅุฐุง ูุชุงุญ
            if depth_map is not None:
                real_distance = depth_estimator.get_object_distance(depth_map, xyxy)
                if real_distance:
                    dist = real_distance
            else:
                # ุงูุทุฑููุฉ ุงููุฏููุฉ ูู fallback
                # ... ุงูููุฏ ุงูุญุงูู ...
```

**ุงููุงุฆุฏุฉ:**
- ๐ ุฏูุฉ ุฃุนูู ุจูุซูุฑ ูู ุงููุณุงูุงุช
- ๐ฏ ุชุญุฐูุฑุงุช ุฃูุซุฑ ููุซูููุฉ
- ๐ ูุนูู ูู ุฃู ุจูุฆุฉ

---

### 1.3 ูุงุฌูุฉ ููุจุงูู ุจุณูุทุฉ (PWA) โญโญ

**ุงูุฃููููุฉ:** ูุชูุณุทุฉ-ุนุงููุฉ  
**ุงูููุช ุงููุชููุน:** 3-5 ุฃูุงู  
**ุงูุตุนูุจุฉ:** ูุชูุณุทุฉ

**ุงููุตู:**
ุชุทุจูู ููุจ ุชูุฏูู (PWA) ูุนูู ุนูู ุฃู ููุจุงูู ุจุฏูู ุชุซุจูุช.

**ุงูุชูููุฐ:**

```html
<!-- ููู ุฌุฏูุฏ: client/index.html -->

<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ูุณุงุนุฏ ุงูููููููู</title>
    <link rel="manifest" href="/manifest.json">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: #1a1a1a;
            color: white;
            height: 100vh;
            display: flex;
            flex-direction: column;
        }
        #video-container {
            flex: 1;
            position: relative;
            background: black;
        }
        #camera-feed {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        #overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
        }
        #controls {
            padding: 20px;
            background: #2a2a2a;
            display: flex;
            gap: 10px;
            justify-content: center;
        }
        button {
            padding: 15px 30px;
            font-size: 18px;
            border: none;
            border-radius: 10px;
            background: #4CAF50;
            color: white;
            cursor: pointer;
            font-weight: bold;
        }
        button:active { background: #45a049; }
        button.danger { background: #f44336; }
        #status {
            padding: 10px;
            text-align: center;
            background: #333;
            font-size: 16px;
        }
        .alert-critical {
            background: #f44336 !important;
            animation: pulse 0.5s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
    </style>
</head>
<body>
    <div id="status">ุฌุงูุฒ ููุจุฏุก</div>
    
    <div id="video-container">
        <video id="camera-feed" autoplay playsinline></video>
        <canvas id="overlay"></canvas>
    </div>
    
    <div id="controls">
        <button id="start-btn" onclick="startAssistant()">ุงุจุฏุฃ</button>
        <button id="stop-btn" onclick="stopAssistant()" class="danger" style="display:none">ุฃููู</button>
    </div>

    <script>
        const API_URL = window.location.origin;
        let stream = null;
        let intervalId = null;
        let synth = window.speechSynthesis;
        
        async function startAssistant() {
            try {
                // ุทูุจ ุงููุงููุฑุง
                stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment' }
                });
                
                document.getElementById('camera-feed').srcObject = stream;
                document.getElementById('start-btn').style.display = 'none';
                document.getElementById('stop-btn').style.display = 'block';
                document.getElementById('status').textContent = 'ูุนูู ุงูุขู...';
                
                // ุจุฏุก ุงูุชุญููู ูู ุซุงููุฉ
                intervalId = setInterval(analyzeFrame, 1000);
                
                speak('ุจุฏุฃ ุงููุณุงุนุฏ');
                
            } catch (err) {
                alert('ุฎุทุฃ ูู ุงููุตูู ูููุงููุฑุง: ' + err.message);
            }
        }
        
        function stopAssistant() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
            if (intervalId) {
                clearInterval(intervalId);
            }
            
            document.getElementById('start-btn').style.display = 'block';
            document.getElementById('stop-btn').style.display = 'none';
            document.getElementById('status').textContent = 'ูุชููู';
            
            speak('ุชููู ุงููุณุงุนุฏ');
        }
        
        async function analyzeFrame() {
            const video = document.getElementById('camera-feed');
            const canvas = document.createElement('canvas');
            canvas.width = 640;
            canvas.height = 480;
            
            const ctx = canvas.getContext('2d');
            ctx.drawImage(video, 0, 0, 640, 480);
            
            // ุชุญููู ูู base64
            const imageData = canvas.toDataURL('image/jpeg', 0.8);
            
            try {
                const response = await fetch(`${API_URL}/infer/realtime`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        event_id: Date.now().toString(),
                        user_id: 'mobile_user',
                        small_image_b64: imageData
                    })
                });
                
                const result = await response.json();
                
                // ุฑุณู ุงููุชุงุฆุฌ
                drawDetections(result.objects);
                
                // ุงูุชูุจูู ุงูุตูุชู
                if (result.speak_message) {
                    speak(result.speak_message);
                }
                
                // ุชุญุฏูุซ ุงูุญุงูุฉ
                updateStatus(result);
                
            } catch (err) {
                console.error('Analysis error:', err);
            }
        }
        
        function drawDetections(objects) {
            const canvas = document.getElementById('overlay');
            const video = document.getElementById('camera-feed');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            
            const ctx = canvas.getContext('2d');
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            objects.forEach(obj => {
                const [x1, y1, x2, y2] = obj.bbox;
                
                // ููู ุญุณุจ ุงููุณุงูุฉ
                let color = 'green';
                if (obj.distance_m < 1) color = 'red';
                else if (obj.distance_m < 2) color = 'orange';
                
                ctx.strokeStyle = color;
                ctx.lineWidth = 3;
                ctx.strokeRect(x1, y1, x2-x1, y2-y1);
                
                // ุงููุต
                ctx.fillStyle = color;
                ctx.font = '20px Arial';
                ctx.fillText(`${obj.class} (${obj.distance_m.toFixed(1)}m)`, x1, y1-5);
            });
        }
        
        function speak(text) {
            // ุฅููุงู ุงูููุงู ุงูุณุงุจู
            synth.cancel();
            
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = 'ar-SA';
            utterance.rate = 1.1;
            utterance.pitch = 1.0;
            
            synth.speak(utterance);
        }
        
        function updateStatus(result) {
            const status = document.getElementById('status');
            const criticalObjects = result.objects.filter(o => o.distance_m < 1.5);
            
            if (criticalObjects.length > 0) {
                status.className = 'alert-critical';
                status.textContent = `โ๏ธ ${criticalObjects.length} ุฃุดูุงุก ูุฑูุจุฉ!`;
            } else {
                status.className = '';
                status.textContent = `${result.objects.length} ุฃุดูุงุก ููุชุดูุฉ`;
            }
        }
    </script>
</body>
</html>
```

**ุงููุงุฆุฏุฉ:**
- ๐ฑ ูุนูู ุนูู ุฃู ููุจุงูู ููุฑุงู
- ๐ค ุชูุจููุงุช ุตูุชูุฉ ุชููุงุฆูุฉ
- ๐๏ธ ูุงุฌูุฉ ุจุตุฑูุฉ ูููุจุตุฑูู ุงููุณุงุนุฏูู

---

## ๐ ุงููุฑุญูุฉ 2: ููุฒุงุช ูุชูุฏูุฉ (1-3 ุฃุดูุฑ)

### 2.1 ุงูุชุนุฑู ุนูู ุงููุตูุต (OCR) โญโญโญ

**ุงูุฃูููุฉ:** ุนุงููุฉ ุฌุฏุงู ููุญูุงุฉ ุงูููููุฉ

**ุงูุงุณุชุฎุฏุงูุงุช:**
- ูุฑุงุกุฉ ุงููุงูุชุงุช
- ุฃุณูุงุก ุงููุญูุงุช
- ุชูุงุฑูุฎ ุงูุตูุงุญูุฉ
- ุฃุฑูุงู ุงูุญุงููุงุช
- ุงูููุงุฆู ูู ุงููุทุงุนู
- ุงูุฃุฏููุฉ

**ุงูุชูููุฐ:**

```python
# ููู ุฌุฏูุฏ: app/vision/ocr_reader.py

import easyocr
from typing import List, Dict

class OCRReader:
    def __init__(self):
        # ุฏุนู ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ
        self.reader = easyocr.Reader(['ar', 'en'], gpu=False)
        print("โ OCR Reader loaded (Arabic + English)")
    
    def read_text(self, image_bytes) -> List[Dict]:
        """ูุฑุงุกุฉ ุงููุตูุต ูู ุตูุฑุฉ"""
        try:
            import cv2
            import numpy as np
            
            arr = np.frombuffer(image_bytes, np.uint8)
            img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            
            # ูุฑุงุกุฉ ุงููุตูุต
            results = self.reader.readtext(img)
            
            texts = []
            for (bbox, text, confidence) in results:
                if confidence > 0.3:  # ููุท ุงููุตูุต ุงููุงุถุญุฉ
                    texts.append({
                        'text': text,
                        'confidence': round(confidence, 2),
                        'bbox': bbox,
                        'language': 'ar' if self._is_arabic(text) else 'en'
                    })
            
            return texts
            
        except Exception as e:
            print(f"โ๏ธ OCR error: {e}")
            return []
    
    def _is_arabic(self, text):
        """ูุดู ุฅุฐุง ูุงู ุงููุต ุนุฑุจู"""
        arabic_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF')
        return arabic_chars > len(text) / 2

ocr_reader = OCRReader()
```

**endpoint ุฌุฏูุฏ:**

```python
# ูู app/vision/router.py

@router.post('/read_text')
async def read_text(file: UploadFile = File(...)):
    """ูุฑุงุกุฉ ุงููุตูุต ูู ุตูุฑุฉ"""
    content = await file.read()
    texts = ocr_reader.read_text(content)
    
    # ุชุฌููุน ุงููุตูุต
    all_text = ' . '.join([t['text'] for t in texts])
    
    return {
        'texts': texts,
        'combined_text': all_text,
        'count': len(texts)
    }
```

---

### 2.2 ุงูุชุนุฑู ุนูู ุงูุนููุงุช ูุงููููุฏ โญโญ

**ุงูุฃูููุฉ:** ุนุงููุฉ ููุงุณุชููุงููุฉ ุงููุงููุฉ

**ุงูุชูููุฐ:**
- ุชุฏุฑูุจ ูููุฐุฌ YOLO ุนูู ุงูุนููุงุช ุงููุญููุฉ
- ูุดู ุงููุฆุงุช (1ุ 5ุ 10ุ 20ุ 50ุ 100ุ 200)
- ุงูุชูููุฒ ุจูู ุงููุฑูู ูุงููุนุฏูู

---

### 2.3 ูุถุน ุงูุชุณูู ุงูุฐูู โญโญ

**ุงูููุฒุงุช:**
- ูุณุญ ุงูุจุงุฑููุฏ
- ูุฑุงุกุฉ ุงูุฃุณุนุงุฑ
- ุงูุชุนุฑู ุนูู ุงูููุชุฌุงุช
- ุชุญุฐูุฑ ูู ุชูุงุฑูุฎ ุงูุตูุงุญูุฉ

---

### 2.4 ุงูููุงุญุฉ ุงูุฏุงุฎููุฉ โญโญโญ

**ุงููุตู:**
ุจูุงุก ุฎุฑูุทุฉ 3D ููุฃูุงูู ุงููุฃูููุฉ (ุงูุจูุชุ ุงูููุชุจ).

**ุงูุชูููุงุช:**
- SLAM (Simultaneous Localization and Mapping)
- ARCore/ARKit ููููุจุงูู
- ุญูุธ ุงููุนุงูู ุงูุซุงุจุชุฉ

---

## ๐ ุงููุฑุญูุฉ 3: ุงูุฐูุงุก ุงููุชูุฏู (3-6 ุฃุดูุฑ)

### 3.1 ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงูุชุญุงุฏุซู โญโญโญ

**ุงููุตู:**
ุฏูุฌ GPT-4 ุฃู Claude ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุนูุฏุฉ.

**ุฃูุซูุฉ:**
- "ูุงุฐุง ุฃูุงููุ" โ ูุตู ุชูุตููู
- "ููู ุฃุตู ููุจุงุจุ" โ ุฅุฑุดุงุฏุงุช ุฎุทูุฉ ุจุฎุทูุฉ
- "ูู ูุฐุง ุขููุ" โ ุชุญููู ุงููุฎุงุทุฑ

---

### 3.2 ุงูุชุนูู ุงููุณุชูุฑ ูุงูุชุฎุตูุต โญโญ

**ุงูููุฒุงุช:**
- ุชุนูู ุนุงุฏุงุช ุงููุณุชุฎุฏู
- ุชุฐูุฑ ุงูุฃูุงูู ุงูููุถูุฉ
- ุชููุน ุงูุงุญุชูุงุฌุงุช
- ุงูุชููู ูุน ุงูุจูุฆุฉ

---

### 3.3 ุงูููุงุญุฉ ุงูุฎุงุฑุฌูุฉ ุจู GPS โญโญโญ

**ุงูููุฒุงุช:**
- ุฏูุฌ ูุน Google Maps
- ุฅุฑุดุงุฏุงุช ุตูุชูุฉ ุฎุทูุฉ ุจุฎุทูุฉ
- ูุดู ุฅุดุงุฑุงุช ุงููุฑูุฑ
- ุชุญุฐูุฑ ูู ุงููุฑูุจุงุช

---

## ๐ ููุงููุณ ุงููุฌุงุญ

### ุงูููุงููุณ ุงูุชูููุฉ:
- โ ุฏูุฉ ุงููุดู: > 90%
- โ ุณุฑุนุฉ ุงูุงุณุชุฌุงุจุฉ: < 300ms
- โ ุฏูุฉ ุงููุณุงูุฉ: ยฑ 20cm
- โ ูุนุฏู ุงูุฃุฎุทุงุก: < 5%

### ุงูููุงููุณ ุงูุจุดุฑูุฉ:
- โ ุฑุถุง ุงููุณุชุฎุฏููู: > 85%
- โ ุงูุงุณุชุฎุฏุงู ุงููููู: > 70%
- โ ุงูุญูุงุฏุซ ุงููุชุฌูุจุฉ: ููุงุณ ููู
- โ ุงูุงุณุชููุงููุฉ: ุฒูุงุฏุฉ ููููุณุฉ

---

## ๐ ุงูุฎูุงุตุฉ

ูุฐุง ุงููุดุฑูุน ูุฏูู ุฅููุงููุงุช ูุงุฆูุฉ ูุชุบููุฑ ุญูุงุฉ ุงูููููููู. ุงูุฃููููุงุช:

1. **ูุธุงู ุงูุชูุจููุงุช ุงูุตูุชูุฉ** - ููุฑู ูุญููู
2. **ุชุญุณูู ุงููุณุงูุฉ ุจู Depth** - ุฏูุฉ ุฃุนูู
3. **ูุงุฌูุฉ ููุจุงูู PWA** - ุณูููุฉ ุงูุงุณุชุฎุฏุงู
4. **OCR ูููุตูุต** - ุถุฑูุฑู ููุญูุงุฉ ุงูููููุฉ
5. **ุงูููุงุญุฉ ุงูุฏุงุฎููุฉ** - ุงุณุชููุงููุฉ ูู ุงูุฃูุงูู ุงููุฃูููุฉ

**ุงูุชุฑููุฒ:** ุงูุณูุงูุฉ ุฃููุงูุ ุซู ุงูุงุณุชููุงููุฉุ ุซู ุงูุฑุงุญุฉ.

**ุงููุจุฏุฃ:** ูู ููุฒุฉ ูุฌุจ ุฃู ุชูุฎุชุจุฑ ูุน ูุณุชุฎุฏููู ุญูููููู ูุจู ุงูุฅุทูุงู.
